{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز سوم پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل:  ساعت ۶ صبح ۸ تیر<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    در این فاز از پروژه، تمرکز ما بر \n",
    "    crawling \n",
    "    و تحلیل مقالات استخراج‌شده از اینترنت خواهد بود. ما با بررسی تکنیک های مختلف  \n",
    "    web crawling\n",
    "    برای استخراج مقالات و سایر اطلاعات مرتبط از وب شروع خواهیم کرد.\n",
    "    <br>\n",
    "    در مرحله بعد، الگوریتم های تجزیه و تحلیل  لینک مانند\n",
    "    PageRank\n",
    "    و \n",
    "    HITS\n",
    "    را برای تعیین اهمیت این مقالات بر اساس نقل قول‌ها، ارجاعات یا اشکال دیگر پیوندها اعمال خواهیم‌کرد. ما همچنین یاد خواهیم‌گرفت که چگونه یک الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده را پیاده‌سازی کنیم که ترجیحات کاربر را برای ارائه نتایج مرتبط تر در نظر می‌گیرد.\n",
    "    <br>\n",
    "    در بخش سوم این مرحله، یک موتور جستجوی شخصی‌سازی شده را پیاده‌سازی میکنیم و یاد می‌گیریم که چگونه موتور جستجویی بسازیم که نتایجی را بر اساس ترجیحات کاربر ارائه دهد.\n",
    "    <br>\n",
    "در نهایت، ما یک \n",
    "    task \n",
    "     در مورد \n",
    "    recommendation system \n",
    "    ها خواهیم‌داشت، که در آن از تکنیک های مختلف برای توصیه مقالات یا صفحات وب به کاربران بر اساس ترجیحات و رفتار آنها استفاده خواهیم کرد.\n",
    "    <br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیاده‌سازی Crawler (۴۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "   در این بخش باید یک Crawler \n",
    "    برای واکشی اطلاعات تعدادی مقاله از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> پیاده سازی کنید.\n",
    "   اطلاعات واکشی شده باید حاوی موارد زیر باشد.\n",
    "</font>\n",
    "</div>\n",
    "<br>\n",
    "<table dir=\"ltr\" style=\"width: 100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication Year</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Authors</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Related Topics</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Citation Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Reference Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">References</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Unique ID of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication year</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of the first author, ..., Name of the last author</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">topic1, topic2, ...</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of citations of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of references of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID of the first reference, ..., ID of the tenth reference</td>\n",
    "  </tr>\n",
    "</table>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  ابتدا فرایند واکشی را از ۵ مقاله‌ی هر استاد شروع کنید و\n",
    "    ۱۰\n",
    "    مرجع اول هر مقاله را به صف مقالات اضافه کنید.\n",
    "    فرایند واکشی را نا جایی ادامه دهید که اطلاعات ۲۰۰۰ مقاله را داشته باشید.\n",
    "    اطلاعات مقالات را در فایل crawled_paper_profName.json ذخیره کنید.\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  در پیاده سازی Crawler به موارد زیر دقت کنید.\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "<li>حق استفاده از api سایت semantic scholar را ندارید.</li>\n",
    "<li>برای واکشی می‌توانید از پکیج‌هایی مثل <a href=\"https://www.selenium.dev/selenium/docs/api/py/\">Selenium</a> و یا <a href=\"https://github.com/scrapy/scrapy\">Scrapy</a>  استفاده کنید. استفاده از پکیج‌های دیگر نیز مجاز است. همچنین برای پارس اطلاعات واکشی شده می‌توانید از پکیج <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a> استفاده کنید.\n",
    "</li>\n",
    "<li>بین هر بار درخواست از سایت یک فاصله چند ثانیه‌ای بدهید.</li>\n",
    "<li>در زمان تحویل کد Crawler شما اجرا خواهد شد و صحت آن بررسی خواهد شد.</li>\n",
    "<li>در صورتی که ‌Crawler شما به دچار اروری مثل request timeout شد نباید کار خود را متوقف کند.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T06:44:18.034044Z",
     "start_time": "2023-06-30T06:44:17.921535Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import datetime\n",
    "import time\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T07:08:56.194480Z",
     "start_time": "2023-06-30T07:08:56.181382Z"
    }
   },
   "outputs": [],
   "source": [
    "class RelatedTopicsDirection(Enum):\n",
    "    NEXT = 'Next'\n",
    "    PREVIOUS = 'Previous'\n",
    "\n",
    "\n",
    "def get_element_by_attribute(driver, tag, value, error_in_number=True, get_all=False, attribute='data-test-id'):\n",
    "    # https://stackoverflow.com/questions/26304224/find-element-by-attribute\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, f'{tag}[{attribute}=\"{value}\"]')\n",
    "    if error_in_number and len(elements) > 1 and not get_all:\n",
    "        raise Exception(f'Invalid number of elements for {driver.current_url}')\n",
    "    elif len(elements) == 0:\n",
    "        return None\n",
    "\n",
    "    return elements if get_all else elements[0]\n",
    "\n",
    "\n",
    "def get_corpus_id(driver):\n",
    "    element = get_element_by_attribute(driver, 'li', 'corpus-id')\n",
    "    return element.text.split(':')[1].strip()\n",
    "\n",
    "\n",
    "def get_id(driver):\n",
    "    url = driver.current_url\n",
    "    id = url.split('/')[-1]\n",
    "    if '#' in id:\n",
    "        id = id.split('#')[0]\n",
    "    return id\n",
    "\n",
    "\n",
    "def get_title(driver):\n",
    "    return get_element_by_attribute(driver, 'h1', 'paper-detail-title').text\n",
    "\n",
    "\n",
    "def toggle_abstract_expansion(driver):\n",
    "    # https://stackoverflow.com/questions/11676790/click-command-in-selenium-webdriver-does-not-work\n",
    "    expand_buttons = get_element_by_attribute(driver, 'button', 'text-truncator-toggle', get_all=True)\n",
    "    if expand_buttons is None:\n",
    "        return False\n",
    "\n",
    "    for expand_button in expand_buttons:\n",
    "        expand_button.send_keys(Keys.RETURN)\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_abstract(driver):\n",
    "    is_toggled = toggle_abstract_expansion(driver)\n",
    "    if not is_toggled:\n",
    "        return None\n",
    "\n",
    "    abstract_element = get_element_by_attribute(driver, 'span', 'text-truncator-text')\n",
    "    if abstract_element is None:\n",
    "        abstract_element = driver.find_element(By.CLASS_NAME, 'paper-detail__abstract__highlighted')\n",
    "        abstract_element_text = abstract_element.text\n",
    "        if abstract_element_text.endswith('.Collapse'):\n",
    "            abstract_element_text = abstract_element.text[:-8]\n",
    "    else:\n",
    "        abstract_element_text = abstract_element.text\n",
    "\n",
    "    abstract = abstract_element_text.strip()\n",
    "    toggle_abstract_expansion(driver)\n",
    "    return abstract\n",
    "\n",
    "\n",
    "def get_publication_year(driver):\n",
    "    # https://www.javatpoint.com/parse-date-from-string-python\n",
    "    date_element = get_element_by_attribute(driver, 'span', 'paper-year')\n",
    "    if date_element is None:\n",
    "        return None\n",
    "    date_str = date_element.text\n",
    "    try:\n",
    "        format = '%d %B %Y'\n",
    "        date_obj = datetime.datetime.strptime(date_str, format)\n",
    "        return date_obj\n",
    "    except:\n",
    "        return date_str\n",
    "\n",
    "\n",
    "def expand_authors(driver):\n",
    "    expand_button = get_element_by_attribute(driver, 'button', 'author-list-expand', False)\n",
    "    if expand_button is None:\n",
    "        return\n",
    "    expand_button.send_keys(Keys.RETURN)\n",
    "\n",
    "\n",
    "def collapse_authors(driver):\n",
    "    collapse_button = get_element_by_attribute(driver, 'button', 'author-list-collapse', False)\n",
    "    if collapse_button is None:\n",
    "        return\n",
    "    collapse_button.send_keys(Keys.RETURN)\n",
    "\n",
    "\n",
    "def get_authors(driver):\n",
    "    expand_authors(driver)\n",
    "    comma_css_selector = 'span[aria-hidden=\"true\"]'\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                    f'.author-list > span[data-test-id=\"author-list\"] > *:not({comma_css_selector})')\n",
    "    authors = [element.text for element in elements]\n",
    "    collapse_authors(driver)\n",
    "    return authors\n",
    "\n",
    "\n",
    "def go_down_until_related_topics(driver):\n",
    "    element = get_element_by_attribute(driver, 'a', 'related-papers', attribute='data-heap-nav')\n",
    "    element.send_keys(Keys.RETURN)\n",
    "\n",
    "\n",
    "def go_to_up_page(driver):\n",
    "    driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.CONTROL, Keys.UP)\n",
    "\n",
    "\n",
    "def change_related_topics(driver, direction: RelatedTopicsDirection):\n",
    "    # https://stackoverflow.com/questions/51336849/selenium-implicit-and-explicit-waits-not-working-has-no-effect\n",
    "\n",
    "    try:\n",
    "        change_related_topics_button = driver.find_element(By.CSS_SELECTOR,\n",
    "                                                           f'button[aria-label=\"{direction.value} related papers\"]')\n",
    "        change_related_topics_button.send_keys(Keys.RETURN)\n",
    "\n",
    "        if direction == RelatedTopicsDirection.NEXT:\n",
    "            time.sleep(1)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_related_topic_bundle(driver):\n",
    "    try:\n",
    "        elements = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[data-test-id=\"title-link\"]'))\n",
    "        )\n",
    "        return [element.text for element in elements]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_related_topics(driver, tries=0):\n",
    "    ALL_NEEDED_TOPICS = 10\n",
    "\n",
    "    go_down_until_related_topics(driver)\n",
    "    is_there_next_page = True\n",
    "    related_topics = []\n",
    "    while len(related_topics) < ALL_NEEDED_TOPICS and is_there_next_page:\n",
    "        related_topic_bundle = get_related_topic_bundle(driver)\n",
    "        if related_topic_bundle is not None:\n",
    "            related_topics += related_topic_bundle\n",
    "        elif tries < 3:\n",
    "            driver.refresh()\n",
    "            return get_related_topics(driver, tries + 1)\n",
    "        else:\n",
    "            return None\n",
    "        is_there_next_page = change_related_topics(driver, RelatedTopicsDirection.NEXT)\n",
    "\n",
    "    is_there_prev_page = True\n",
    "    while is_there_prev_page:\n",
    "        is_there_prev_page = change_related_topics(driver, RelatedTopicsDirection.PREVIOUS)\n",
    "\n",
    "    go_to_up_page(driver)\n",
    "    return related_topics\n",
    "\n",
    "\n",
    "def get_citation_count(driver):\n",
    "    try:\n",
    "        element = driver.find_element(By.CSS_SELECTOR, '.scorecard-stat__headline')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return int(element.text.split()[0].strip().replace(',', ''))\n",
    "\n",
    "\n",
    "def get_references_count(driver):\n",
    "    try:\n",
    "        element = driver.find_element(By.CSS_SELECTOR, 'a[data-heap-nav=\"cited-papers\"]')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return int(element.text.split()[0].strip().replace(',', ''))\n",
    "\n",
    "\n",
    "def get_references_id(driver):\n",
    "    # https://stackoverflow.com/questions/12340737/specify-multiple-attribute-selectors-in-css\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                    'a[data-test-id=\"citation-paper-title\"][data-heap-citation-type=\"citedPapers\"]')\n",
    "    return [element.get_attribute('data-heap-paper-id') for element in elements][:10]\n",
    "\n",
    "\n",
    "def get_references_url(driver):\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                    'a[data-test-id=\"citation-paper-title\"][data-heap-citation-type=\"citedPapers\"]')\n",
    "    return [element.get_attribute('href') for element in elements][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T06:44:29.335215Z",
     "start_time": "2023-06-30T06:44:29.324687Z"
    }
   },
   "outputs": [],
   "source": [
    "class Paper:\n",
    "    def __init__(self, url, corpus_id, id, title, abstract, publication_year, authors, related_topics, citation_count,\n",
    "                 reference_count, references_id, references_url):\n",
    "        self.url = url\n",
    "        self.corpus_id = corpus_id\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "        self.abstract = abstract\n",
    "        self.publication_year = publication_year\n",
    "        self.authors = authors\n",
    "        self.related_topics = related_topics\n",
    "        self.citation_count = citation_count\n",
    "        self.reference_count = reference_count\n",
    "        self.references_id = references_id\n",
    "        self.references_url = references_url\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Paper({self.url}, {self.corpus_id}, {self.id}, {self.title}, {self.abstract}, {self.publication_year}, {self.authors}, {self.related_topics}, {self.citation_count}, {self.reference_count}, {self.references_id}, {self.references_url})'\n",
    "\n",
    "\n",
    "def create_paper_class(url):\n",
    "    global Paper\n",
    "\n",
    "    driver.get(url)\n",
    "    if '502 Bad Gateway' in driver.title:\n",
    "        time.sleep(5)\n",
    "        driver.get(url)\n",
    "\n",
    "    corpus_id = get_corpus_id(driver)\n",
    "    id = get_id(driver)\n",
    "    title = get_title(driver)\n",
    "    abstract = get_abstract(driver)\n",
    "    publication_year = get_publication_year(driver)\n",
    "    authors = get_authors(driver)\n",
    "    related_topics = get_related_topics(driver)\n",
    "    citation_count = get_citation_count(driver)\n",
    "    reference_count = get_references_count(driver)\n",
    "    references_id = get_references_id(driver)\n",
    "    references_url = get_references_url(driver)\n",
    "\n",
    "    return Paper(url, corpus_id, id, title, abstract, publication_year, authors, related_topics, citation_count,\n",
    "                 reference_count, references_id, references_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T13:25:59.623607Z",
     "start_time": "2023-06-30T13:25:58.356956Z"
    }
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T06:45:18.735034Z",
     "start_time": "2023-06-30T06:45:18.721097Z"
    }
   },
   "outputs": [],
   "source": [
    "url_queue = []\n",
    "professor_names = ['Kasaei', 'Rabiee', 'Rohban', 'Sharifi', 'Soleymani']\n",
    "initial_file_names = [f'{professor_name}.txt' for professor_name in professor_names]\n",
    "\n",
    "\n",
    "def initialize_url_queue():\n",
    "    global url_queue, initial_file_names\n",
    "\n",
    "    for file_name in initial_file_names:\n",
    "        with open(file_name, 'r') as file:\n",
    "            for line in file:\n",
    "                url_queue.append(line.strip())\n",
    "\n",
    "\n",
    "url_done = []\n",
    "all_papers = []\n",
    "initialize_url_queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T13:28:04.000330Z",
     "start_time": "2023-06-30T13:26:05.150703Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching https://www.semanticscholar.org/paper/ABySS%3A-a-parallel-assembler-for-short-read-sequence-Simpson-Wong/3f6ec7c3909ee1f88b45fa6dfecc95024546c679: 100%|██████████| 10/10 [01:58<00:00, 11.88s/it]\n"
     ]
    }
   ],
   "source": [
    "FETCH_PAPERS_COUNT = 2000 - len(url_done)\n",
    "pbar = tqdm(range(FETCH_PAPERS_COUNT))\n",
    "\n",
    "for _ in pbar:\n",
    "    url = url_queue[0]\n",
    "    pbar.set_description(f'Fetching {url}')\n",
    "    paper = create_paper_class(url)\n",
    "    all_papers.append(paper)\n",
    "    # to help the host!\n",
    "    time.sleep(0.5)\n",
    "    url_queue += paper.references_url\n",
    "    url_queue = url_queue[1:]\n",
    "    url_done.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "2000"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_papers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:28:44.548633Z",
     "start_time": "2023-06-30T13:28:44.540780Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T13:28:46.867326Z",
     "start_time": "2023-06-30T13:28:46.853464Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = 'crawled_paper_profName.json'\n",
    "\n",
    "\n",
    "def json_default(value):\n",
    "    if isinstance(value, datetime.date):\n",
    "        return dict(year=value.year, month=value.month, day=value.day)\n",
    "    else:\n",
    "        return value.__dict__\n",
    "\n",
    "\n",
    "def serialize_papers():\n",
    "    global Paper, file_name\n",
    "\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(all_papers, file, default=json_default, indent=4)\n",
    "\n",
    "\n",
    "def deserialize_papers():\n",
    "    global file_name\n",
    "\n",
    "    with open(file_name, 'r') as file:\n",
    "        papers_array = json.load(file)\n",
    "        papers = [Paper(**paper) for paper in papers_array]\n",
    "        for paper in papers:\n",
    "            try:\n",
    "                paper.publication_year = datetime.datetime(paper.publication_year['year'],\n",
    "                                                           paper.publication_year['month'],\n",
    "                                                           paper.publication_year['day'])\n",
    "            except:\n",
    "                paper.publication_year = paper.publication_year\n",
    "\n",
    "        return papers\n",
    "\n",
    "def serialize_queues():\n",
    "    global url_queue, url_done, file_name\n",
    "\n",
    "    with open(file_name.split('.')[0] + '_queue.json', 'w') as file:\n",
    "        json.dump([url_queue, url_done], file, default=json_default, indent=4)\n",
    "\n",
    "def deserialize_queues():\n",
    "    global file_name\n",
    "\n",
    "    with open(file_name.split('.')[0] + '_queue.json', 'r') as file:\n",
    "        url_queue, url_done = json.load(file)\n",
    "        return url_queue, url_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T13:28:49.105420Z",
     "start_time": "2023-06-30T13:28:48.838476Z"
    }
   },
   "outputs": [],
   "source": [
    "serialize_papers()\n",
    "serialize_queues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T13:29:22.443106Z",
     "start_time": "2023-06-30T13:29:22.366786Z"
    }
   },
   "outputs": [],
   "source": [
    "deserialized_all_papers = deserialize_papers()\n",
    "deserialized_url_queue, deserialized_url_done = deserialize_queues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "all_papers = deserialize_papers()\n",
    "url_queue, url_done = deserialize_queues()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T06:45:30.414737Z",
     "start_time": "2023-06-30T06:45:30.375424Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>PageRank \n",
    "        شخصی‌سازی‌شده\n",
    "        (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش، الگوریتم \n",
    "    PageRank \n",
    "    شخصی‌سازی‌شده را پیاده‌سازی می‌کنیم که توسعه‌ای از الگوریتم \n",
    "    PageRank\n",
    "    است که ترجیحات کاربر را در نظر می‌گیرد. الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده گره‌ها را در یک گراف بر اساس اهمیت آنها برای کاربر رتبه‌بندی می‌کند، نه بر اساس اهمیت کلی آنها در نمودار.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:24.179974Z",
     "start_time": "2023-06-30T13:31:24.174442Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:25.362840Z",
     "start_time": "2023-06-30T13:31:25.333465Z"
    }
   },
   "outputs": [],
   "source": [
    "def pagerank(graph: Dict[str, List[str]]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Returns the personalized PageRank scores for the nodes in the graph, given the user's preferences.\n",
    "\n",
    "    Parameters:\n",
    "    graph (Dict[str, List[str]]): The graph represented as a dictionary of node IDs and their outgoing edges.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, float]: A dictionary of node IDs and their personalized PageRank scores.\n",
    "    \"\"\"\n",
    "\n",
    "    queue_to_see = [node for node in graph.keys()]\n",
    "    scores: Dict[str, float] = {}\n",
    "    visited = set()\n",
    "\n",
    "    # For each reference to the node, add 1 to the score\n",
    "    while len(queue_to_see) > 0:\n",
    "        current_node = queue_to_see.pop(0)\n",
    "        for node in graph[current_node]:\n",
    "            if node not in visited:\n",
    "                queue_to_see.append(node)\n",
    "                visited.add(node)\n",
    "                scores[node] = 1\n",
    "            else:\n",
    "                scores[node] += 1\n",
    "\n",
    "    # Normalization\n",
    "    # for node in scores.keys():\n",
    "    #     scores[node] = scores[node] / len(graph.keys())\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش از الگوریتم \n",
    "PageRank\n",
    "شخصی‌سازی‌شده که در قسمت قبلی پیاده‌سازی شده‌است برای\n",
    "شناسایی مقالات مهم مرتبط با حوزه‌ی کاری یک استاد \n",
    "خاص استفاده می‌کنیم. این تابع، یک \n",
    "    field \n",
    "    را به عنوان ورودی دریافت می‌کند. خروجی نیز\n",
    "مقالات برتری که بیشترین ارتباط را با آن زمینه دارند؛ خواهدبود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:27.204676Z",
     "start_time": "2023-06-30T13:31:27.188718Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_professor_papers(professor: str) -> List[Paper]:\n",
    "    global all_papers\n",
    "    professor_papers: List[Paper] = []\n",
    "    professor_papers_id: List[str] = []\n",
    "\n",
    "    # Add all papers that the professor has written\n",
    "    for paper in all_papers:\n",
    "        for author in paper.authors:\n",
    "            if professor in author:\n",
    "                professor_papers.append(paper)\n",
    "                professor_papers_id.append(paper.id)\n",
    "\n",
    "    # Add all papers that the professor has referenced\n",
    "    for professor_paper in professor_papers:\n",
    "        for reference in professor_paper.references_id:\n",
    "            if reference not in professor_papers_id:\n",
    "                professor_papers_id.append(reference)\n",
    "\n",
    "    return professor_papers\n",
    "\n",
    "\n",
    "def get_professor_graph(professor: str) -> Dict[str, List[str]]:\n",
    "    professor_papers = get_professor_papers(professor)\n",
    "    graph: Dict[str, List[str]] = {}\n",
    "\n",
    "    for paper in professor_papers:\n",
    "        graph[paper.id] = []\n",
    "        for reference in paper.references_id:\n",
    "            graph[paper.id].append(reference)\n",
    "            if graph.get(reference) is None:\n",
    "                graph[reference] = []\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def important_articles(professor: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns the most important articles in the field of given professor, based on the personalized PageRank scores.\n",
    "\n",
    "    Parameters:\n",
    "    Professor (str): Professor's name.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of article IDs representing the most important articles in the field of given professor.\n",
    "    \"\"\"\n",
    "\n",
    "    graph = get_professor_graph(professor)\n",
    "    scores = pagerank(graph)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:27.596520Z",
     "start_time": "2023-06-30T13:31:27.579412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imporant articles for Kasaei:\n",
      " [('d5c6edb53dc41f298f145041cd2c53e40e3acf2b', 5), ('b5bd72d8bc9f51dae65c15842f0ab443c3b437e3', 4), ('577d19a115f9ef6f002483fcf88adbb3b5479556', 4), ('f3a11158e9d8bdfdf07dca756335c084fce0123e', 4), ('05f63bdf9e60d0a299cfe5e8d7ba043904f1fea1', 3), ('ab67b9d0da50e251a4f7e42370540547b891ceb1', 3), ('3fed78dcdcf2588f2f1b34ad6885a60789574203', 2), ('4e4504e867e867f8b2bc366b75e036686582e5bf', 2), ('78b4f65c167185f18b573433e5f3e8814acf656f', 2), ('a9407584b7641f70bf0882e495ddef561d0ee62b', 2)]\n"
     ]
    }
   ],
   "source": [
    "professor_important_articles = important_articles(professor_names[0])\n",
    "print(f'imporant articles for {professor_names[0]}:\\n', professor_important_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو شخصی‌سازی‌شده (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "الگوریتم جست‌و‌جویی که در فازهای گذشته پیاده‌سازی کرده‌اید را به گونه‌ای تغییر دهید که نتایج به دست آمده جست‌و‌جو بر حسب علایق فرد مرتب شوند. از قضیه‌ی خطی بودن برای این کار استفاده کنید.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:29.267712Z",
     "start_time": "2023-06-30T13:31:29.249564Z"
    }
   },
   "outputs": [],
   "source": [
    "def fake_search(title_query: str, abstract_query: str, max_result_count: int, method: str = 'ltn-lnn',\n",
    "                weight: float = 0.5,\n",
    "                should_print=False, preferred_field: str = None):\n",
    "    global all_papers\n",
    "\n",
    "    return random.choices(all_papers, k=max_result_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:29.927257Z",
     "start_time": "2023-06-30T13:31:29.914130Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def new_search(title_query: str, abstract_query: str, max_result_count: int, method: str = 'ltn-lnn',\n",
    "               weight: float = 0.5,\n",
    "               should_print=False, preferred_field: List = None):\n",
    "    \"\"\"\n",
    "        Finds relevant documents to query\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        max_result_count: Return top 'max_result_count' docs which have the highest scores. \n",
    "                          notice that if max_result_count = -1, then you have to return all docs\n",
    "        \n",
    "        mode: 'detailed' for searching in title and text separately.\n",
    "              'overall' for all words, and weighted by where the word appears on.\n",
    "        \n",
    "        where: when mode ='detailed', when we want search query \n",
    "                in title or text not both of them at the same time.\n",
    "        \n",
    "        method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
    "        \n",
    "        preferred_field: A list containing preference rate to Dr. Rabiee, Dr. Soleymani, Dr. Rohban, \n",
    "                         Dr. Kasaei, and Dr. Sharifi's papers, respectively.\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------------------------------------------------------------------\n",
    "        list\n",
    "        Retrieved documents with snippet\n",
    "    \"\"\"\n",
    "    global all_papers, professor_names\n",
    "\n",
    "    search_results = fake_search(title_query=title_query, abstract_query=abstract_query,\n",
    "                                 max_result_count=max_result_count,\n",
    "                                 method=method, weight=weight, should_print=should_print)\n",
    "    professors_page_rank = {}\n",
    "    for professor in professor_names:\n",
    "        professors_page_rank[professor] = pagerank(get_professor_graph(professor))\n",
    "\n",
    "    preferred_field_professor_names = ['Rabiee', 'Soleymani', 'Rohban', 'Kasaei', 'Sharifi']\n",
    "\n",
    "    custom_page_rank = {}\n",
    "    for paper in search_results:\n",
    "        custom_page_rank[paper.id] = 0\n",
    "        for professor in professor_names:\n",
    "            if paper.id in professors_page_rank[professor]:\n",
    "                custom_page_rank[paper.id] += professors_page_rank[professor][paper.id] * preferred_field[\n",
    "                    preferred_field_professor_names.index(professor)]\n",
    "\n",
    "    sorted_custom_page_rank = sorted(custom_page_rank.items(), key=lambda x: x[1], reverse=True)\n",
    "    result = []\n",
    "    # sort search results based on custom page rank\n",
    "    for page_rank_paper in sorted_custom_page_rank:\n",
    "        if page_rank_paper in search_results:\n",
    "            result.append(page_rank_paper)\n",
    "\n",
    "    for search_result in search_results:\n",
    "        if search_result not in result:\n",
    "            result.append(search_result)\n",
    "\n",
    "    return [result.id for result in result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:30.646300Z",
     "start_time": "2023-06-30T13:31:30.630654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['a23937d6345c6e707cf17509ec27c76fef4ce7cc',\n '3f116f1763a3604275b06c6ccf0dcd65910d13b5',\n 'a4cec122a08216fe8a3bc19b22e78fbaea096256',\n '5c5be36e3111e42247d78a6d529e4b1d7d2ced12',\n 'e5c33c8df40dd55cb3068b759a30f76bbd8b9933',\n '0d095dcf4fb6e4cc701db75c7a2c1076440d3df8',\n '923562d216386a88947d40da310d94bbb1376a41',\n 'c51d7cbfb95ee370d1eddb4e0ff03290b8bb479a',\n '2e99b1868621e47cb89ba8e0f72a5b9d87acb991',\n '5a9aebb4c6836595bd8d38a06b19197e1ea02a69']"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_search(title_query='deep learning', abstract_query='deep learning', max_result_count=10,\n",
    "           preferred_field=[0.3, 0.2, 0.2, 0.1, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رتبه‌بندی نویسندگان (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>  \n",
    "    برای رتبه‌بندی نویسندگان، مفهوم ارجاع نویسندگان به یکدیگر مطرح می‌شود. زمانی که نویسنده A در مقاله خود به مقاله P که نویسنده B جزو نویسندگان آن مقاله یعنی مقاله P می‌باشد، ارجاع دهد، می‌گوییم که نویسنده A به نویسنده B ارجاع داده است. با توجه به این رابطه، می‌توان گراف ارجاعات بین نویسندگان را ایجاد و سپس با استفاده از الگوریتم HITS\n",
    "نویسندگان را رتبه‌بندی کرد. برای رتبه‌بندی نیاز است تا از شاخص‌های hub و authority استفاده کنیم.\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:41.823841Z",
     "start_time": "2023-06-30T13:31:40.488161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M. Kristan', 'Jiri Matas']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def get_authors_graph(papers):\n",
    "    id_to_paper_dict = {}\n",
    "    for paper in papers:\n",
    "        id_to_paper_dict[paper.id] = paper\n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "    for paper in papers:\n",
    "        for reference_id in paper.references_id:\n",
    "            reference_paper = id_to_paper_dict.get(reference_id, None)\n",
    "            if reference_paper is None:\n",
    "                continue\n",
    "\n",
    "            for author in paper.authors:\n",
    "                for reference_author in reference_paper.authors:\n",
    "                    graph.add_edge(author, reference_author)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def hit_algorithm(papers, n):\n",
    "    \"\"\"\n",
    "        Implementing the HITS algorithm to score authors based on their papers and co-authors.\n",
    "\n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        papers: A list of paper dictionaries with the following keys:\n",
    "                \"id\": A unique ID for the paper\n",
    "                \"title\": The title of the paper\n",
    "                \"abstract\": The abstract of the paper\n",
    "                \"date\": The year in which the paper was published\n",
    "                \"authors\": A list of the names of the authors of the paper\n",
    "                \"related_topics\": A list of IDs for related topics (optional)\n",
    "                \"citation_count\": The number of times the paper has been cited (optional)\n",
    "                \"reference_count\": The number of references in the paper (optional)\n",
    "                \"references\": A list of IDs for papers that are cited in the paper (optional)\n",
    "        n: An integer representing the number of top authors to return.\n",
    "\n",
    "        Returns\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        List \n",
    "        list of the top n authors based on their hub scores.\n",
    "    \"\"\"\n",
    "    # Create a graph of authors and papers (all the authors and papers represented as nodes, and all the authors who wrote each paper connected to the corresponding paper node by an edge)\n",
    "    graph = get_authors_graph(papers)\n",
    "\n",
    "    # Run the HITS algorithm\n",
    "    hubs, authorities = nx.hits(graph)\n",
    "\n",
    "    # Create a list of top n authors based on their hub scores\n",
    "    top_authors = sorted(hubs, key=hubs.get, reverse=True)[:n]\n",
    "    return top_authors\n",
    "\n",
    "\n",
    "# call the hit_algorithm function\n",
    "top_authors = hit_algorithm(all_papers, 2)\n",
    "\n",
    "# print the top authors\n",
    "print(top_authors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>سیستم پیشنهادگر (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سعی می‌کنیم که یک سیستم پیشنهادگر مقالات بر اساس جست‌و‌جو‌ها یا علايق یک کاربر پیاده‌سازی کنیم، سیستم پیشنهاد دهنده‌ای که قصد داریم آن را ایجاد کنیم،‌ باید بتواند بر اساس لیستی از مقالاتی که کاربر قبلا آن‌ها را مطالعه کرده یا به آن‌ها علاقه داشته است، مقالات تازه انتشار یافته‌‌ی جدید را به کاربر پیشنهاد دهد.\n",
    "\n",
    "در فایل recommended_papers.json\n",
    "لیستی از کاربران قرار دارد که در فیلد positive_papers هر کاربر،\n",
    "تعداد ۵۰ مقاله از مقالاتی که کاربر به آن‌ها علاقه داشته است مشخص شده است. و همچینین در فیلد recommendedPapers هر کاربر تعداد ۱۰ مقاله به ترتیب اهمیت، از مقالات جدیدی که کاربر آن‌ها را پسندیده است قرار دارد.\n",
    "\n",
    "در این بخش هدف شما یادگیری سیستم پیشنهاد‌ دهنده بر اساس همین داده‌ها می‌باشد، و به عبارتی شما بایستی کاربر‌ها را به دو دسته آموزش و آزمایش تقسیم کنید، و بر اساس داده‌های آموزشی بتوانید مقالات جدید مورد پسند کاربرهای آزمایش را پیش‌بینی کنید. (بنابراین در این پیش‌بینی نمی‌توانید از فیلد recommendedPapers این کاربران استفاده کنید.)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:49.396776Z",
     "start_time": "2023-06-30T13:31:49.128975Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('recommended_papers.json', 'r') as fp:\n",
    "    recommended_papers = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:49.396947Z",
     "start_time": "2023-06-30T13:31:49.392002Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_user = recommended_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:50.259127Z",
     "start_time": "2023-06-30T13:31:50.254061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d9404b4a794c07b5e2cdf3203aabf06d70c6be9b\n",
      "CENTAURO: A Hybrid Locomotion and High Power Resilient Manipulation Platform\n",
      "Despite the development of a large number of mobile manipulation robots, very few platforms can demonstrate the required strength and mechanical sturdiness to accommodate the needs of real-world applications with high payload and moderate/harsh physical interaction demands, e.g., in disaster-response scenarios or heavy logistics/collaborative tasks. In this letter, we introduce the design of a wheeled-legged mobile manipulation platform capable of executing demanding manipulation tasks, and demonstrating significant physical resilience while possessing a body size (height/width) and weight compatible to that of a human. The achieved performance is the result of combining a number of design and implementation principles related to the actuation system, the integration of body structure and actuation, and the wheeled-legged mobility concept. These design principles are discussed, and the solutions adopted for various robot components are detailed. Finally, the robot performance is demonstrated in a set of experiments validating its power and strength capability when manipulating heavy payload and executing tasks involving high impact physical interactions.\n",
      "['Computer Science']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user['positive_papers'][0]['paperId'])\n",
    "print(sample_user['positive_papers'][0]['title'])\n",
    "print(sample_user['positive_papers'][0]['abstract'])\n",
    "print(sample_user['positive_papers'][0]['fieldsOfStudy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:50.704860Z",
     "start_time": "2023-06-30T13:31:50.687191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94eebbefe8a37cf394be899b85af295c2e3a1f01\n",
      "Efficient Parametric Approximations of Neural Network Function Space Distance\n",
      "It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting mislabeled examples without expensive iterations over the entire dataset.\n",
      "['Computer Science', 'Mathematics']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user['recommendedPapers'][0]['paperId'])\n",
    "print(sample_user['recommendedPapers'][0]['title'])\n",
    "print(sample_user['recommendedPapers'][0]['abstract'])\n",
    "print(sample_user['recommendedPapers'][0]['fieldsOfStudy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Collaborative Filtering (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این راهکار سعی می‌کنیم با استفاده از کاربران مشابه با یک کاربر، سلیقه‌ی او را حدس بزنیم و مقالاتی را که کاربران مشابه دیده‌اند را به کاربر نمایش دهیم.\n",
    "\n",
    "در این روش ابتدا باید $N$ کاربر که سلیقه‌ی مشابه با کاربر $x$ دارند را پیدا کنید، و با ترکیب لیست مقالات جدید مورد علاقه‌ی آن $N$ کاربر مشابه،\n",
    " ۱۰ مقاله‌ به کاربر $x$ پیشنهاد دهید.\n",
    "\n",
    "توجه داشته باشید که برای اینکه شباهت دو کاربر را پیدا کنید، باید cosine_similarity بین بردار زمینه‌های مورد علاقه‌ی دو کاربر استفاده کنید. این بردار از $M$ درایه تشکیل شده است، که $M$ تعداد زمینه‌های یکتاییست که در داده‌ها وجود دارد. و در این بردار درایه‌ی $j$ام\n",
    "نشان دهنده‌ی نسبت تعداد مقالات خوانده‌ی شده‌ کاربر در زمینه‌ی $j$ به تعداد کل مقاله‌های خوانده شده توسط او می‌باشد. (توجه کنید که هر مقاله می‌تواند چند زمینه داشته باشد و بنابراین حاصل جمع درایه‌های این بردار الزاما یک نمی‌باشد)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def get_unique_fields_of_study():\n",
    "    global recommended_papers\n",
    "    fields_of_study = set()\n",
    "\n",
    "    for user in recommended_papers:\n",
    "        positive_papers = user['positive_papers']\n",
    "        for positive_paper in positive_papers:\n",
    "            study_field = positive_paper['fieldsOfStudy']\n",
    "            if study_field is not None:\n",
    "                fields_of_study.update(study_field)\n",
    "\n",
    "        my_recommended_papers = user['recommendedPapers']\n",
    "        for recommended_paper in my_recommended_papers:\n",
    "            study_field = recommended_paper['fieldsOfStudy']\n",
    "            if study_field is not None:\n",
    "                fields_of_study.update(study_field)\n",
    "\n",
    "    return list(fields_of_study)\n",
    "\n",
    "\n",
    "fields_of_study = get_unique_fields_of_study()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:52.310700Z",
     "start_time": "2023-06-30T13:31:52.295139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "recommended_papers_dict = {}\n",
    "for i in range(len(recommended_papers)):\n",
    "    recommended_papers_dict[i] = recommended_papers[i]\n",
    "\n",
    "test_size = 0.2 * len(recommended_papers_dict)\n",
    "train_recommended_papers, test_recommended_papers = {}, {}\n",
    "user_ids = list(recommended_papers_dict.keys())\n",
    "random.seed(0)\n",
    "for i in range(int(test_size)):\n",
    "    random_user_id = random.choice(user_ids)\n",
    "    random_user = recommended_papers_dict[random_user_id]\n",
    "    train_recommended_papers[random_user_id] = random_user\n",
    "\n",
    "for user_id in recommended_papers_dict.keys():\n",
    "    if user_id not in train_recommended_papers:\n",
    "        test_recommended_papers[user_id] = recommended_papers_dict[user_id]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:52.944528Z",
     "start_time": "2023-06-30T13:31:52.940759Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:53.487266Z",
     "start_time": "2023-06-30T13:31:53.481098Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_positive_papers_in_field(user_id, field):\n",
    "    global recommended_papers_dict\n",
    "    field_papers = []\n",
    "    all_read_papers = []\n",
    "    user = recommended_papers_dict[user_id]\n",
    "\n",
    "    positive_papers = user['positive_papers']\n",
    "    for positive_paper in positive_papers:\n",
    "        all_read_papers.append(positive_paper)\n",
    "        study_field = positive_paper['fieldsOfStudy']\n",
    "        if study_field is not None and field in study_field:\n",
    "            field_papers.append(positive_paper)\n",
    "\n",
    "    return len(field_papers), len(all_read_papers)\n",
    "\n",
    "\n",
    "def get_user_vector(user_id):\n",
    "    global fields_of_study\n",
    "    user_vector = [0] * len(fields_of_study)\n",
    "\n",
    "    for study_field in fields_of_study:\n",
    "        read_papers_in_field, all_read_papers = read_positive_papers_in_field(user_id, study_field)\n",
    "        study_field_index = fields_of_study.index(study_field)\n",
    "        user_vector[study_field_index] = read_papers_in_field / all_read_papers\n",
    "\n",
    "    return user_vector\n",
    "\n",
    "\n",
    "def cosine_similarity(user1_vector, user2_vector):\n",
    "    dot_product = 0\n",
    "    user1_norm = 0\n",
    "    user2_norm = 0\n",
    "\n",
    "    for i in range(len(user1_vector)):\n",
    "        dot_product += user1_vector[i] * user2_vector[i]\n",
    "        user1_norm += user1_vector[i] ** 2\n",
    "        user2_norm += user2_vector[i] ** 2\n",
    "\n",
    "    user1_norm = user1_norm ** 0.5\n",
    "    user2_norm = user2_norm ** 0.5\n",
    "\n",
    "    denominator = user1_norm * user2_norm\n",
    "    return dot_product / denominator if denominator != 0 else 0\n",
    "\n",
    "\n",
    "def get_cosine_similarity(user_id1, user_id2):\n",
    "    global fields_of_study\n",
    "    user1_vector = get_user_vector(user_id1)\n",
    "    user2_vector = get_user_vector(user_id2)\n",
    "\n",
    "    return cosine_similarity(user1_vector, user2_vector)\n",
    "\n",
    "\n",
    "def get_top_N_similar_users(user_id: int, N=10):\n",
    "    global train_recommended_papers, fields_of_study\n",
    "    cosine_similarity_list = []\n",
    "\n",
    "    for i in train_recommended_papers.keys():\n",
    "        if i != user_id:\n",
    "            current_cosine_similarity = get_cosine_similarity(user_id, i)\n",
    "            cosine_similarity_list.append((i, current_cosine_similarity))\n",
    "\n",
    "    cosine_similarity_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_N_similar_users = cosine_similarity_list[:N]\n",
    "    return top_N_similar_users\n",
    "\n",
    "\n",
    "def get_predict_score_of_user_to_product(user_id, paper):\n",
    "    global fields_of_study\n",
    "    user_vector = get_user_vector(user_id)\n",
    "    paper_vector = [0] * len(fields_of_study)\n",
    "\n",
    "    study_field = paper['fieldsOfStudy']\n",
    "    if study_field is not None:\n",
    "        for field in study_field:\n",
    "            study_field_index = fields_of_study.index(field)\n",
    "            paper_vector[study_field_index] = 1\n",
    "\n",
    "    return cosine_similarity(user_vector, paper_vector)\n",
    "\n",
    "\n",
    "def collaborative_filtering(user_id: int, N=10):\n",
    "    \"\"\"\n",
    "    Returns the top 10 related articles to the user, based on similar users (Similar users should be on \"train data\").\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): The unique index of the user.\n",
    "    N: The number of hyperparameter N in Nearest Neighbor algorithm.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
    "    \"\"\"\n",
    "\n",
    "    top_N_similar_users = get_top_N_similar_users(user_id, N)\n",
    "    top_N_similar_users = [user[0] for user in top_N_similar_users]\n",
    "\n",
    "    all_recommended_papers = []\n",
    "    for in_user_id in top_N_similar_users:\n",
    "        user_recommended_papers = train_recommended_papers[in_user_id]['recommendedPapers']\n",
    "        for paper in user_recommended_papers:\n",
    "            score = get_predict_score_of_user_to_product(user_id, paper) * get_cosine_similarity(user_id, in_user_id)\n",
    "            all_recommended_papers.append((score, paper))\n",
    "\n",
    "    all_recommended_papers.sort(key=lambda x: x[0], reverse=True)\n",
    "    all_recommended_papers_not_duplicate = []\n",
    "    all_recommended_papers_not_duplicate_id = []\n",
    "    for paper in all_recommended_papers:\n",
    "        paper_id = paper[1]['paperId']\n",
    "\n",
    "        if paper_id not in all_recommended_papers_not_duplicate_id:\n",
    "            all_recommended_papers_not_duplicate.append(paper[1])\n",
    "            all_recommended_papers_not_duplicate_id.append(paper_id)\n",
    "\n",
    "        if len(all_recommended_papers_not_duplicate) == 10:\n",
    "            break\n",
    "\n",
    "    return all_recommended_papers_not_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('7d03af1ccf5404e23bee02903b41850e88cc8590',\n  'Deep Learning on Implicit Neural Representations of Shapes'),\n ('b2db53ac752045905063eb497a6a484627b037b1',\n  'Deep Active Learning in the Presence of Label Noise: A Survey'),\n ('62f3ecee1135503bb2cab776e915281521ef2f3a',\n  'Probabilistic Attention based on Gaussian Processes for Deep Multiple Instance Learning'),\n ('682ff0690c87a31c6bc148e53f56b5b494621d66',\n  'Non-Parametric Outlier Synthesis'),\n ('ebca36a78971096cca26e1e59931dc07bba4b25a', 'Equiangular Basis Vectors'),\n ('79f43d149cd569abf46428ed8a27a8a2b3e44a8f',\n  'A modern look at the relationship between sharpness and generalization'),\n ('1d36e7ba19be5db9694ed256ea21dae5f753ede3',\n  'Trainable Projected Gradient Method for Robust Fine-tuning'),\n ('638e508b1a0694c847d3094ad664006b2b761f09',\n  'An Empirical Analysis of the Shift and Scale Parameters in BatchNorm'),\n ('555e1eb9a171b70309c84d5c5db71e78e255c403',\n  'Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness'),\n ('0dd2dafa9389f83160a63be3fde23b5d121a4786',\n  'DDP: Diffusion Model for Dense Visual Prediction')]"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user_id = random.choice(list(test_recommended_papers.keys()))\n",
    "print(test_user_id)\n",
    "collaborative_filtering_papers = collaborative_filtering(test_user_id, 10)\n",
    "[(paper['paperId'], paper['title']) for paper in collaborative_filtering_papers]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:31:55.980605Z",
     "start_time": "2023-06-30T13:31:55.947103Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Content Based (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این روش با استفاده از مقالات قبلی که کاربر آن‌ها را پسندیده است، به کاربر مقاله‌ی جدید پیشنهاد می‌دهیم.\n",
    "\n",
    "برای اینکار ابتدا تمام مقالات پیشنهاد شده برای تمام کاربرها را سر جمع کنید. (در واقع مدلی که پیاده‌سازی می‌کنید نباید بداند که به کدام کاربر چه مقالاتی پیشنهاد شده است)\n",
    "\n",
    "سپس بردار tf-idf برای تایتل هر یک از مقالات را ایجاد کنید، و میانگین بردار مقالات مورد علاقه‌ی هر فرد را با لیستی که از مقالات جدید سر جمع کردید مقایسه کنید و ۱۰ تا از شبیه‌ترین مقالات را خروجی دهید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:32:03.517224Z",
     "start_time": "2023-06-30T13:32:03.495100Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_recommended_papers():\n",
    "    global recommended_papers_dict\n",
    "\n",
    "    all_recommended_papers = []\n",
    "    for user_id in recommended_papers_dict.keys():\n",
    "        user_recommended_papers = recommended_papers_dict[user_id]['recommendedPapers']\n",
    "        all_recommended_papers += user_recommended_papers\n",
    "\n",
    "    all_recommended_papers_not_duplicate = []\n",
    "    all_recommended_papers_not_duplicate_id = []\n",
    "    for paper in all_recommended_papers:\n",
    "        paper_id = paper['paperId']\n",
    "\n",
    "        if paper_id not in all_recommended_papers_not_duplicate_id:\n",
    "            all_recommended_papers_not_duplicate.append(paper)\n",
    "            all_recommended_papers_not_duplicate_id.append(paper_id)\n",
    "\n",
    "    return all_recommended_papers_not_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "TfidfVectorizer()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "all_recommended_papers = get_all_recommended_papers()\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf_vectorizer.fit([paper['title'] for paper in all_recommended_papers])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:32:04.244685Z",
     "start_time": "2023-06-30T13:32:04.221671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_tf_idf_vector(text):\n",
    "    global tf_idf_vectorizer\n",
    "\n",
    "    tf_idf_vector = tf_idf_vectorizer.transform([text]).toarray()[0]\n",
    "    return tf_idf_vector\n",
    "\n",
    "def get_user_average_positive_papers_vector(user_id):\n",
    "    global recommended_papers_dict\n",
    "\n",
    "    user_positive_papers = recommended_papers_dict[user_id]['positive_papers']\n",
    "    user_positive_papers_vectors = [get_tf_idf_vector(paper['title']) for paper in user_positive_papers]\n",
    "    user_average_positive_papers_vector = np.mean(user_positive_papers_vectors, axis=0)\n",
    "    return user_average_positive_papers_vector\n",
    "\n",
    "def content_based_recommendation(user_id):\n",
    "    \"\"\"\n",
    "    Returns the top 10 related articles to the user, based on the titles of the articles.\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): The unique index of the user.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
    "    \"\"\"\n",
    "    global recommended_papers_dict\n",
    "\n",
    "    user_average_positive_papers_vector = get_user_average_positive_papers_vector(user_id)\n",
    "    all_recommended_papers = get_all_recommended_papers()\n",
    "\n",
    "    scored_papers = []\n",
    "    for paper in all_recommended_papers:\n",
    "        score = cosine_similarity(user_average_positive_papers_vector, get_tf_idf_vector(paper['title']))\n",
    "        scored_papers.append((score, paper))\n",
    "\n",
    "    scored_papers.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_10_papers = [paper[1] for paper in scored_papers[:10]]\n",
    "    return top_10_papers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:32:05.662182Z",
     "start_time": "2023-06-30T13:32:05.655731Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('12ffb095891a42dd80625667a3234ded06ffa040',\n  'BOOSTING THE PERFORMANCE OF DEEP LEARNING: A GRADIENT BOOSTING APPROACH TO TRAINING CONVOLUTIONAL AND DEEP NEURAL NETWORKS'),\n ('8d0f15973385160da6a63465124580f9c49130ad',\n  'Topological Deep Learning: A Review of an Emerging Paradigm'),\n ('b981f7655c236bdd501dc5b9f234df01597a1b8c',\n  'Evaluation of GAN-Based Model for Adversarial Training'),\n ('11528ef970e60416295ce7767bacae492603cfdb',\n  'Deep learning for video-text retrieval: a review'),\n ('9f9f5f10e804cbec812a8f8639769e9a03dee44c',\n  'TransCODE: Co-design of Transformers and Accelerators for Efficient Training and Inference'),\n ('c39f3752591181899f680644e8f6ce77cc5a8e3e',\n  'On the Stability and Generalization of Triplet Learning'),\n ('ba121a6e2583c5f9b137f04324c25239c63d3473',\n  'Bag of Tricks for In-Distribution Calibration of Pretrained Transformers'),\n ('cbe3519785e86efafa3f4f72e7ff1a0b9398529a',\n  'A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation'),\n ('6625a66e96821efed9b19e81d86bda7d66931020',\n  'Domain Generalization in Machine Learning Models for Wireless Communications: Concepts, State-of-the-Art, and Open Issues'),\n ('957d6d3f88d54273f44cb3c3335da209bf9657b9',\n  'A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold')]"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user_id = random.choice(list(recommended_papers_dict.keys()))\n",
    "print(test_user_id)\n",
    "content_based_recommendation_papers = content_based_recommendation(test_user_id)\n",
    "[(paper['paperId'], paper['title']) for paper in content_based_recommendation_papers]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:32:07.008702Z",
     "start_time": "2023-06-30T13:32:06.516984Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>ارزیابی سیستم‌های پیشنهادگر</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سیستم‌های پیشنهادگری را که پیاده کرده‌اید را با استفاده از معیار nDCG و با استفاده از دادگان واقعی از علایق کاربران نسبت به مقالات جدید ارزیابی کنید و نتایج حاصل از دو روش را با هم مقایسه کنید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "test_user_id = random.choice(list(recommended_papers_dict.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:32:12.157098Z",
     "start_time": "2023-06-30T13:32:12.136012Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:32:13.796516Z",
     "start_time": "2023-06-30T13:32:13.780172Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ndcg_score(recommended_papers_id, real_recommended_papers_id):\n",
    "    static_number = 10\n",
    "\n",
    "    recommended_papers_id = recommended_papers_id[:static_number]\n",
    "    real_recommended_papers_id = real_recommended_papers_id[:static_number]\n",
    "\n",
    "    dcg = 0\n",
    "    for i in range(len(recommended_papers_id)):\n",
    "        paper_id = recommended_papers_id[i]\n",
    "        if paper_id in real_recommended_papers_id:\n",
    "            if i == 0:\n",
    "                demoninator = 1\n",
    "            else:\n",
    "                demoninator = np.log2(i + 1)\n",
    "            dcg += (static_number - real_recommended_papers_id.index(paper_id)) / demoninator\n",
    "\n",
    "    idcg = 0\n",
    "    for i in range(len(real_recommended_papers_id)):\n",
    "        if i == 0:\n",
    "            demoninator = 1\n",
    "        else:\n",
    "            demoninator = np.log2(i + 1)\n",
    "        idcg += (static_number - i) / demoninator\n",
    "\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "def calculate_ndcg_for_user(user_id, should_print):\n",
    "    real_recommended_papers = recommended_papers_dict[user_id]['recommendedPapers']\n",
    "    real_recommended_papers_id = [paper['paperId'] for paper in real_recommended_papers]\n",
    "\n",
    "    collaborative_filtering_recommendation_papers = collaborative_filtering(user_id)\n",
    "    collaborative_filtering_recommendation_papers_id = [paper['paperId'] for paper in collaborative_filtering_recommendation_papers]\n",
    "\n",
    "    content_based_recommendation_papers = content_based_recommendation(user_id)\n",
    "    content_based_recommendation_papers_id = [paper['paperId'] for paper in content_based_recommendation_papers]\n",
    "\n",
    "    ndcg_collaborative_filtering = get_ndcg_score(collaborative_filtering_recommendation_papers_id, real_recommended_papers_id)\n",
    "    ndcg_content_based = get_ndcg_score(content_based_recommendation_papers_id, real_recommended_papers_id)\n",
    "\n",
    "    if should_print:\n",
    "        print(f'User ID: {user_id}')\n",
    "        print(f'Collaborative Filtering: {ndcg_collaborative_filtering}')\n",
    "        print(f'Content Based: {ndcg_content_based}')\n",
    "\n",
    "    return ndcg_collaborative_filtering, ndcg_content_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID: 33\n",
      "Collaborative Filtering: 0.10379482759919627\n",
      "Content Based: 0.11292241834839045\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.10379482759919627, 0.11292241834839045)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_ndcg_for_user(test_user_id, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:32:15.883514Z",
     "start_time": "2023-06-30T13:32:14.891040Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 542/542 [06:13<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative Filtering: 0.3598006542201136\n",
      "Content Based: 0.04497527296931547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_user_id = 0\n",
    "max_user_id = len(recommended_papers_dict)\n",
    "ndcg_collaborative_filtering_scores = []\n",
    "ndcg_content_based_scores = []\n",
    "\n",
    "pbar = tqdm(range(min_user_id, max_user_id))\n",
    "for user_id in pbar:\n",
    "    ndcg_collaborative_filtering, ndcg_content_based = calculate_ndcg_for_user(user_id, False)\n",
    "    ndcg_collaborative_filtering_scores.append(ndcg_collaborative_filtering)\n",
    "    ndcg_content_based_scores.append(ndcg_content_based)\n",
    "\n",
    "print(f'Collaborative Filtering: {np.mean(ndcg_collaborative_filtering_scores)}')\n",
    "print(f'Content Based: {np.mean(ndcg_content_based_scores)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:38:32.442181Z",
     "start_time": "2023-06-30T13:32:18.508356Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For one random user, it seems that content based recommendation is better than collaborative filtering. But for all users, collaborative filtering is better than content based recommendation. However collaborative filtering could be implemented in a better way."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رابط کاربری (تا ۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش\n",
    " باید یک واسط کاربری ساده برای اجرای تعاملی بخش‌های مختلف سیستم که از فاز ۱ ساخته‌اید و همچنین مشاهده نتایج پیاده‌سازی کنید. در صورت پیاده سازی زیبا و بهتر رابط کاربری تا ده نمره نمره امتیازی نیز در نظر گرفته خواهد شد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-30T13:39:19.290579Z",
     "start_time": "2023-06-30T13:38:32.445197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ir_phase_1.ipynb\n",
      "Start\n",
      "Start preprocessing ...\n",
      "Indexing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Completed documents: 100%|██████████| 6183/6183 [00:12<00:00, 503.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start viewing ...\n",
      "Start dynamic indexing ...\n",
      "Empty DataFrame\n",
      "Columns: [paperId, title, abstract, Unnamed: 3, Unnamed: 4]\n",
      "Index: []\n",
      "\n",
      "Start compressing index ...\n",
      "Start query correction ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Strings with created bigram index: 100%|██████████| 35993/35993 [00:00<00:00, 40298.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start searching ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Documents with calculated length in title field: 100%|██████████| 6183/6183 [00:04<00:00, 1505.89it/s]\n",
      "Documents with calculated length in abstract field: 100%|██████████| 6183/6183 [00:04<00:00, 1505.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start evaluating...\n",
      "End\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<module 'ir_phase_1' from 'ir_phase_1.ipynb'>"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import ir_phase_1\n",
    "import importlib\n",
    "\n",
    "importlib.reload(ir_phase_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "from ir_phase_1 import SearchResult, search as ir_phase_1_search"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:39:19.294979Z",
     "start_time": "2023-06-30T13:39:19.290993Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# search(title_query: str, abstract_query: str, max_result_count: int, method: str = 'ltn.lnn', weight: float = 0.5, should_print=False)\n",
    "# 'ltn.lnn' or 'ltc.lnc' or 'okapi25'\n",
    "# final_score = weight * abstract_score + (1 - weight) * title_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:39:19.299265Z",
     "start_time": "2023-06-30T13:39:19.297338Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, status, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins='*',\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class Search(BaseModel):\n",
    "    title_query: str\n",
    "    abstract_query: str\n",
    "    max_result_count: int\n",
    "    method: str\n",
    "    weight: float\n",
    "\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search(search: Search):\n",
    "    if search.max_result_count < -1:\n",
    "        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"max_result_count should be greater than -1\")\n",
    "    if search.method not in ['ltn.lnn', 'ltc.lnc', 'okapi25']:\n",
    "        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"method should be one of ltn.lnn, ltc.lnc, okapi25\")\n",
    "    if search.weight < 0 or search.weight > 1:\n",
    "        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"weight should be between 0 and 1\")\n",
    "\n",
    "    try:\n",
    "        search_results = ir_phase_1_search(search.title_query, search.abstract_query, search.max_result_count, search.method, search.weight, False)\n",
    "        return json.dumps(search_results, default=lambda o: o.__dict__, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:39:19.578023Z",
     "start_time": "2023-06-30T13:39:19.304180Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [29997]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "Checked documents: 100%|██████████| 1139/1139 [00:02<00:00, 496.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:57593 - \"POST /search HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No posting list for term: hiiiiiiii (probably a stop word)\n",
      "No posting list for term: hiiiiiiii (probably a stop word)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checked documents: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:57630 - \"POST /search HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [29997]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T13:46:51.708136Z",
     "start_time": "2023-06-30T13:43:42.946322Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The Frontend part is attached as search_engin_ui."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
